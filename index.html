<!DOCTYPE HTML>
<html lang="en">

<style>
  body {
    font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
    -webkit-font-smoothing: antialiased;
    -moz-osx-font-smoothing: grayscale;
  }
</style>

  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Jerry (Qilong) Cheng</title>
    <meta name="author" content="Jerry Cheng">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
  </head>


  <body>
    <body>
      <table style="width:100%;max-width:800px;margin:auto;border-spacing:0px;">
      <tbody>
      <tr>
      <td style="padding:2.5%;width:63%;vertical-align:middle">
      <p class="name" style="text-align:center;font-size:36px;font-weight:bold;">Jerry (Qilong) Cheng</p>
      <p style="font-size:15px;">
      I am a <b>Research Fellow</b> at <a href="https://haosu-robotics.github.io">New York University’s Biomechatronics and Intelligent Robotics Lab</a>, advised by <b>Prof. Hao Su</b>. I previously conducted research at the <a href="https://starslab.ca">Space and Terrestrial Autonomous Robotics Systems Lab</a> with <b>Prof. Jonathan Kelly</b>, the <a href="https://danielwigdor.com">Dynamic Graphics Project Lab</a> with <b>Prof. Daniel Wigdor</b>, and the <a href="https://robotics.utoronto.ca">University of Toronto Robotics Institute</a> with <b>Profs. Matthew Mackay</b> and <b>Ali Bereyhi</b>.
      </p>

      <!-- <p style="margin-top:18px;font-size:15px;">
      <b><u>Goal:</u></b> Build robots that <b>empower creativity and intelligence</b>—advancing filmmaking, locomotion, and human–robot collaboration.
      </p> -->

      <p style="margin-top:18px;font-size:15px;">
      <b><u>Research Interest:</u></b> The intersection of <b>robotics</b>, <b>large-scale reinforcement learning</b>, and <b>vision-based control</b>—toward unified frameworks for <b>loco-manipulation</b> and <b>cinematic motion planning</b>.
      </p>

      <p style="margin-top:18px;font-size:15px;">
      <b><u>Research Question:</u></b> How can we design <b>scalable robot learning flywheels</b> that unify perception, whole-body control, and dexterous manipulation—enabling general-purpose, reliable robots in unstructured real-world environments?
      </p>

      <!-- <p style="margin-top:18px;font-size:15px;">
      <b><u>Robots:</u></b> I love building humanoids and cinematic arms that move and perceive as fluidly as humans—bridging art, intelligence, and motion.
      </p> -->

      <p style="margin-top:18px;font-size:15px;">
      <b>Email:</b> <a href="mailto:qilong.cheng@mail.utoronto.ca">qilong.cheng@mail.utoronto.ca</a>
      </p>

      <p style="text-align:center;margin-top:24px;">
      <a href="mailto:qilong.cheng@mail.utoronto.ca">Email</a> &nbsp;/&nbsp;
      <a href="data/resume.pdf">CV</a> &nbsp;/&nbsp;
      <a href="https://scholar.google.com/citations?user=iQuHS3MAAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
      <a href="https://www.linkedin.com/in/jerry-qilong-cheng-540039163/">LinkedIn</a> &nbsp;/&nbsp;
      <a href="https://github.com/thejerrycheng">GitHub</a>
      </p>
      </td>
      <td style="padding:2.5%;width:37%;max-width:37%">
      <a href="assets/images/profile_pic.png"><img style="width:100%;border-radius:50%;box-shadow:0 0 10px rgba(0,0,0,0.2);" src="assets/images/profile_pic.png" alt="profile photo"></a>
      </td>
      </tr>
      </tbody>
      </table>
      </body>

    <table style="width:100%;max-width:800px;margin:auto;border-spacing:0px;">
      <tbody>
        <tr><td><h2>Education</h2>
          <p><b><a href="https://www.nyu.edu" target="_blank">New York University</a></b>, Research Fellowship – Biomechatronics and Intelligent Robotics Lab<br>
          <i>Sep 2025 – Aug 2026</i></p>
          <p><b><a href="https://www.utoronto.ca" target="_blank">University of Toronto</a></b>, M.Eng. in Computer Engineering (Robotics), GPA: 3.89/4.00<br>
          <i>Sep 2023 – Aug 2025</i></p>
          <p><b><a href="https://www.utoronto.ca" target="_blank">University of Toronto</a></b>, B.A.Sc. in Mechanical Engineering, Minor in Robotics & Business, Senior GPA: 3.92/4.00<br>
          <i>Aug 2017 – Jun 2023</i></p>
        </td></tr>
      </tbody>
    </table>

    <!-- ======= Research Interests Section ======= -->

    <table style="width:100%;max-width:800px;margin:auto;border-spacing:0px;">
      <tbody>
        <tr><td><h2>Research Interests</h2>
          <p>My research focuses on integrating learning-based control and perception for robots. Specifically, I develop reinforcement learning algorithms for visual loco-manipulation, imitation learning for exoskeletons, and vision-based control frameworks for biped and manipulator robots. I am also exploring unified policies that bridge hardware-aware and perception-driven intelligence.</p>
        </td></tr>
      </tbody>
    </table>

    <!-- ======= News Section ======= -->
    <table style="width:100%;max-width:800px;margin:auto;border-spacing:0px;margin-top:30px;">
      <tbody>
        <tr>
          <!-- Left side: News text -->
          <td style="padding:8px;vertical-align:middle;width:65%;">
            <h2 style="margin-bottom:8px;">News</h2>
            <p style="font-size:15px;line-height:1.5em;">
              <b>[10/20/2025]</b> Attended IROS 2025 in Hangzhou, China to present our paper <b>"AeroHaptix"</b>
            </p>
          </td>

          <!-- Right side: Image carousel (one-at-a-time) -->
          <td style="padding:8px;width:35%;vertical-align:middle;text-align:center;">
            <div style="position:relative;width:100%;height:200px;overflow:hidden;border-radius:6px;">
              <!-- Track -->
              <div id="newsCarousel"
                  style="display:flex;transition:transform 0.5s ease; height:200px;">
                <!-- Each slide takes full container width -->
                <div style="min-width:100%;height:200px;display:flex;align-items:center;justify-content:center;">
                  <img src="assets/images/news/iros_2025/iros_sign.jpg" alt="IROS"
                      style="height:200px;width:auto;max-width:100%;display:block;">
                </div>
                <div style="min-width:100%;height:200px;display:flex;align-items:center;justify-content:center;">
                  <img src="assets/images/news/iros_2025/group_pic.jpg" alt="Group Pic"
                      style="height:200px;width:auto;max-width:100%;display:block;">
                </div>
                <div style="min-width:100%;height:200px;display:flex;align-items:center;justify-content:center;">
                  <img src="assets/images/news/iros_2025/unitree.jpg" alt="Unitree"
                      style="height:200px;width:auto;max-width:100%;display:block;">
                </div>
              </div>

              <!-- Navigation buttons -->
              <button onclick="prevImage()" aria-label="Previous image"
                      style="position:absolute;top:50%;left:5px;transform:translateY(-50%);
                            background-color:rgba(0,0,0,0.5);color:white;border:none;padding:4px 8px;
                            border-radius:4px;cursor:pointer;">&#10094;</button>
              <button onclick="nextImage()" aria-label="Next image"
                      style="position:absolute;top:50%;right:5px;transform:translateY(-50%);
                            background-color:rgba(0,0,0,0.5);color:white;border:none;padding:4px 8px;
                            border-radius:4px;cursor:pointer;">&#10095;</button>
            </div>
          </td>
        </tr>
      </tbody>
    </table>

    <script>
      let currentIndex = 0;
      const carousel = document.getElementById('newsCarousel');
      const slides = Array.from(carousel.children);
      const totalSlides = slides.length;

      function showImage(index) {
        currentIndex = ((index % totalSlides) + totalSlides) % totalSlides; // wrap
        carousel.style.transform = `translateX(-${currentIndex * 100}%)`;
      }

      function nextImage() { showImage(currentIndex + 1); }
      function prevImage() { showImage(currentIndex - 1); }

      // Keep alignment after a resize
      window.addEventListener('resize', () => showImage(currentIndex));
    </script>



<!-- ======= Publications Section ======= -->
<style>
  /* Font: prefer Apple Helvetica / Helvetica Neue */
  :root { --pubs-font: "Helvetica Neue", Helvetica, Arial, sans-serif; }

  .pubs-table,
  .pubs-table td,
  .pubs-table h2,
  .pub-meta,
  .pub-meta a,
  .pub-desc {
    font-family: var(--pubs-font);
    -webkit-font-smoothing: antialiased;
    -moz-osx-font-smoothing: grayscale;
  }

  /* Existing styles (keep these) */
  .pubs-table { width:100%; max-width:800px; margin:auto; border-spacing:0 10px; border-collapse:separate; }
  .pub-row td { vertical-align:middle; }
  .pub-thumb { position:relative; width:100%; max-width:220px; }
  .pub-thumb img { width:100%; display:block; border-radius:6px; }
  .pub-thumb .hover-video {
    position:absolute; inset:0; opacity:0; transition:opacity .25s ease;
    border-radius:6px; overflow:hidden;
  }
  .pub-row:hover .hover-video { opacity:1; }
  .pub-meta a { text-decoration:none; }
  .pub-meta .papertitle { font-weight:600; }
  .pub-desc { margin-top:6px; font-size:14px; line-height:1.4em; }
</style>


<table class="pubs-table">
  <tbody>
    <tr><td><h2>Publications</h2></td></tr>

    <!-- Pub 1 -->
    <tr class="pub-row" data-pub="1">
      <td style="padding:5px; width:40%; text-align:center;">
        <div class="pub-thumb">
          <img src="assets/images/publications/pub1.jpg" alt="Robot-World & Hand-Eye Calibration thumbnail" loading="lazy">
        </div>
      </td>
      <td class="pub-meta" style="padding:2px; width:100%;">
        <a href="https://arxiv.org/abs/2409.17420" target="_blank" rel="noopener noreferrer">
          <span class="papertitle">A Certifiably Correct Algorithm for Generalized Robot-World and Hand-Eye Calibration</span>
        </a><br>
        E. Wise, P. Kaveti, <b>Q. Cheng</b>, W. Wang, H. Singh, J. Kelly, D. M. Rosen, and M. Giamou<br>
        <em>International Journal of Robotics Research (IJRR)</em>, 2025<br>
        <a href="https://arxiv.org/abs/2409.17420" target="_blank" rel="noopener noreferrer">arXiv</a>
        <p class="pub-desc">
          Introduces a certifiably correct calibration algorithm ensuring globally optimal robot-world and hand-eye alignment under general conditions.
        </p>
      </td>
    </tr>

    <!-- Pub 2 -->
    <tr class="pub-row" data-pub="2">
      <td style="padding:16px; width:20%; text-align:center;">
        <div class="pub-thumb">
          <img src="assets/images/publications/pub2.jpg" alt="VibraForge thumbnail" loading="lazy">
          <div class="hover-video">
            <video width="100%" height="100%" muted autoplay loop playsinline>
              <source src="assets/videos/publications/pub2.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </td>
      <td class="pub-meta" style="padding:8px; width:80%;">
        <a href="https://arxiv.org/abs/2409.17420" target="_blank" rel="noopener noreferrer">
          <span class="papertitle">VibraForge: A Scalable Prototyping Toolkit For Creating Spatialized Vibrotactile Feedback Systems</span>
        </a><br>
        B. Huang, S. Ren, Y. Luo, <b>Q. Cheng</b>, H. Cai, Y. Sang, M. Sousa, P. H. Dietz, and D. Wigdor<br>
        <em>CHI</em>, 2024<br>
        <a href="https://arxiv.org/abs/2409.17420" target="_blank" rel="noopener noreferrer">arXiv</a>
        <p class="pub-desc">
          An open-source toolkit enabling rapid design and deployment of large-scale vibrotactile feedback systems for spatialized haptic interactions.
        </p>
      </td>
    </tr>

    <!-- Pub 3 -->
    <tr class="pub-row" data-pub="3">
      <td style="padding:16px; width:20%; text-align:center;">
        <div class="pub-thumb">
          <img src="assets/images/publications/pub3.jpg" alt="AeroHaptix thumbnail" loading="lazy">
          <div class="hover-video">
            <video width="100%" height="100%" muted autoplay loop playsinline>
              <source src="assets/videos/publications/pub3.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </td>
      <td class="pub-meta" style="padding:8px; width:80%;">
        <a href="https://arxiv.org/abs/2407.12105" target="_blank" rel="noopener noreferrer">
          <span class="papertitle">AeroHaptix: A Wearable Vibrotactile Feedback System for Enhancing Collision Avoidance in UAV Teleoperation</span>
        </a><br>
        B. Huang, Z. Wang, <b>Q. Cheng</b>, S. Ren, H. Cai, A. Alvarez Valdivia, K. Mahadevan, and D. Wigdor<br>
        <em>IEEE RA-L</em>, 2024<br>
        <a href="https://arxiv.org/abs/2407.12105" target="_blank" rel="noopener noreferrer">arXiv</a>
        <p class="pub-desc">
          Introduces a wearable vibrotactile system integrated with UAV teleoperation, leveraging control barrier functions for safe human-drone interaction.
        </p>
      </td>
    </tr>

    <!-- Pub 4 -->
    <tr class="pub-row" data-pub="4">
      <td style="padding:16px; width:20%; text-align:center;">
        <div class="pub-thumb">
          <img src="assets/images/publications/pub4.jpg" alt="Radar Pair Calibration thumbnail" loading="lazy">
          <div class="hover-video">
            <video width="100%" height="100%" muted autoplay loop playsinline>
              <source src="assets/videos/publications/pub4.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </td>
      <td class="pub-meta" style="padding:8px; width:80%;">
        <a href="https://ieeexplore.ieee.org/document/10196187" target="_blank" rel="noopener noreferrer">
          <span class="papertitle">Extrinsic Calibration of 2D Millimetre-Wavelength Radar Pairs Using Ego-Velocity Estimates</span>
        </a><br>
        <b>Q. Cheng</b>, E. Wise, and J. Kelly<br>
        <em>IEEE AIM</em>, 2023<br>
        <a href="https://ieeexplore.ieee.org/document/10196187" target="_blank" rel="noopener noreferrer">IEEE</a>
        <p class="pub-desc">
          Proposes a calibration framework for radar-radar pairs leveraging ego-motion cues without overlapping views for improved pose estimation accuracy.
        </p>
      </td>
    </tr>

    <!-- Pub 5 -->
    <tr class="pub-row" data-pub="5">
      <td style="padding:16px; width:20%; text-align:center;">
        <div class="pub-thumb">
          <img src="assets/images/publications/pub5.jpg" alt="Radar-Camera Spatiotemporal Calibration thumbnail" loading="lazy">
          <div class="hover-video">
            <video width="100%" height="100%" muted autoplay loop playsinline>
              <source src="assets/videos/publications/pub5.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </td>
      <td class="pub-meta" style="padding:8px; width:80%;">
        <a href="https://ieeexplore.ieee.org/abstract/document/10256219" target="_blank" rel="noopener noreferrer">
          <span class="papertitle">Spatiotemporal Calibration of 3D Millimetre-Wavelength Radar-Camera Pairs</span>
        </a><br>
        E. Wise, <b>Q. Cheng</b>, and J. Kelly<br>
        <em>IEEE TRO</em>, 2022<br>
        <a href="https://ieeexplore.ieee.org/abstract/document/10256219" target="_blank" rel="noopener noreferrer">IEEE</a>
        <p class="pub-desc">
          Introduces continuous-time B-spline calibration for radar-camera systems, outperforming state-of-the-art methods in robustness and precision.
        </p>
      </td>
    </tr>

    <!-- Pub 6 -->
    <tr class="pub-row" data-pub="6">
      <td style="padding:16px; width:20%; text-align:center;">
        <div class="pub-thumb">
          <img src="assets/images/publications/pub6.jpg" alt="Weakly Supervised Augmentation thumbnail" loading="lazy">
          <div class="hover-video">
            <video width="100%" height="100%" muted autoplay loop playsinline>
              <source src="assets/videos/publications/pub6.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </td>
      <td class="pub-meta" style="padding:8px; width:80%;">
        <a href="https://ieeexplore.ieee.org/document/9745100" target="_blank" rel="noopener noreferrer">
          <span class="papertitle">Weakly Supervised Semantic and Attentive Data Mixing Augmentation for Fine-Grained Visual Categorization</span>
        </a><br>
        M. He, <b>Q. Cheng</b>, and G. Qi<br>
        <em>IEEE Access</em>, 2022<br>
        <a href="https://ieeexplore.ieee.org/document/9745100" target="_blank" rel="noopener noreferrer">IEEE</a>
        <p class="pub-desc">
          Proposes a weakly supervised attention-guided data augmentation strategy for fine-grained image classification tasks.
        </p>
      </td>
    </tr>

    <!-- Pub 7 -->
    <tr class="pub-row" data-pub="7">
      <td style="padding:16px; width:20%; text-align:center;">
        <div class="pub-thumb">
          <img src="assets/images/publications/pub7.jpg" alt="Generative Design for Unicycle Robot thumbnail" loading="lazy">
          <div class="hover-video">
            <video width="100%" height="100%" muted autoplay loop playsinline>
              <source src="assets/videos/publications/pub7.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </td>
      <td class="pub-meta" style="padding:8px; width:80%;">
        <a href="https://nanolithography.spiedigitallibrary.org/conference-proceedings-of-spie/12253/122530A/Generative-design-for-self-balancing-unicycle-robot-in-additive-manufacturing/10.1117/12.2639454.short" target="_blank" rel="noopener noreferrer">
          <span class="papertitle">Generative Design for Self-Balancing Unicycle Robot in Additive Manufacturing</span>
        </a><br>
        J. Chen, <b>Q. Cheng</b>, and M. Han<br>
        <em>SPIE ACAIB</em>, 2022<br>
        <a href="https://nanolithography.spiedigitallibrary.org/conference-proceedings-of-spie/12253/122530A/Generative-design-for-self-balancing-unicycle-robot-in-additive-manufacturing/10.1117/12.2639454.short" target="_blank" rel="noopener noreferrer">SPIE</a>
        <p class="pub-desc">
          Applies generative design and additive manufacturing techniques to develop a self-balancing unicycle robot prototype.
        </p>
      </td>
    </tr>

  </tbody>
</table>


<!-- ======= Research Experience Section ======= -->

    </table>

    <table style="width:100%;max-width:800px;margin:auto;border-spacing:0px;">
      <tbody>
        <tr><td><h2>Research Experience</h2>
          <ul>
            <li><b>Research Fellow</b> – Biomechatronics and Intelligent Robotics Lab, NYU (Prof. Hao Su) <i>(Sep 2025 – Present)</i><br>Developed reinforcement learning and musculoskeletal simulation frameworks for exoskeleton and humanoid locomotion benchmarking.</li>
            <li><b>Robotics Researcher</b> – STARS Lab, University of Toronto (Prof. Jonathan Kelly) <i>(Sep 2021 – Aug 2025)</i><br>Developed radar-based calibration and state estimation algorithms validated on real-world driving datasets.</li>
            <li><b>HCI Researcher</b> – Dynamic Graphics Project Lab, UofT (Prof. Daniel Wigdor) <i>(Sep 2022 – Aug 2025)</i><br>Created open-source vibrotactile prototyping and wearable haptics systems integrated with UAV teleoperation.</li>
            <li><b>M.Eng Researcher</b> – UofT Robotics Institute <i>(Sep 2024 – Aug 2025)</i><br>Designed and built IRIS: a modular 6-DOF 3D-printed cinema robot arm with imitation learning and visual planning.</li>
          </ul>
        </td></tr>
      </tbody>
    </table>

    <!-- ======= Technical Skills Section ======= -->

    <table style="width:100%;max-width:800px;margin:auto;border-spacing:0px;">
      <tbody>
        <tr><td><h2>Technical Skills</h2>
          <p><b>Software & Tools:</b> Python, C++, MATLAB, ROS, MuJoCo, Isaac Sim, IsaacLab, PyTorch, Docker, Linux<br>
          <b>Hardware & Robots:</b> xArm, Unitree, Raspberry Pi, Jetson, Intel RealSense, Arduino, STM32, PND Adam, Booster Robot</p>
        </td></tr>
      </tbody>
    </table>
  </body>
</html>
